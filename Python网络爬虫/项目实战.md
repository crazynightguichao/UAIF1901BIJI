# 第5节：项目实战
### 凤凰网
```
scrapy框架:

ifengspider
```
### 百度图片
```
# 案例一、爬取图片
# 导入
# from urllib.request import *        # urllib 是一个文件夹，request是一个模块
# from lxml import etree
# import time

# url = "http://unsplash.lofter.com/"
#
# with urlopen(url) as html:
#     # print(html.geturl())        # 没有重定向
#     # print(html.info())
#     # print(html.getcode())
#
#     text = html.read().decode("utf-8")
#
# doc = etree.HTML(text)      # 模拟的
# links = doc.xpath("//div[contains(@class,'m-post-img')]/a/span/img/@src")       # 12 个链接
# # print(links)
#
# # 下载资源
# for i in range(len(links)):
#     time.sleep(1)
#     print("正在下载第%s个"%i)
#     urlretrieve(links[i],'img/%s.jpg'%i)        # 下载资源的函数
```
### 网易云音乐
```
# 案例二、爬取音乐
# from urllib.request import *        # urllib 是一个文件夹，request是一个模块
# from lxml import etree
# import time
# # from fake_useragent import UserAgent        # 插件，获取报头的一个库：'User-Agent': ua.random  'User-Agent': 'ua.chrome'
# # ua = UserAgent()
#
# url = "https://music.163.com/artist?id=2116"
# URL2 = "https://music.163.com/song/media/outer/url?id="
#
# headers = {  # 请求报头
#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
#     # 'User-Agent': ua.random
#     # 'User-Agent': 'ua.chrome'
# }
#
# req = Request(url,headers=headers)       # urllib的请求对象  urlopen（url|RequestObject）
#
# with urlopen(req) as html:
#     text = html.read().decode('utf-8')
#
# # 获取
# doc = etree.HTML(text)
# # print(text)     # 反爬机制，没有给服务器发送东西，就会被当作爬虫
#
# songs = doc.xpath("//ul[@class='f-hide']/li/a/text()")
# links = doc.xpath("//ul[@class='f-hide']/li/a/@href")
# ids = [ link[9::] for link in links]
# # print(list(zip(songs,ids)))
#
#
# for sid,title in zip(ids,songs):
#     time.sleep(1)
#     req2 = Request(URL2 + str(sid),headers=headers)
#     with urlopen(req2) as html:
#         urlretrieve(html.geturl(),'songs/%s.mp3'%title)     # 下载资源的函数
#         print("songs/%s.mp3 下载完成"%title)

```
### 豆瓣电影
```
# 例四 电影的名字和链接
from lxml import etree
from urllib.request import *
import pickle     # 持久化
import time
arr = []

# url = "https://movie.douban.com/top250?start="
# urls = [url+str(i) for i in range(0,250,25)]
#
# def spider(link):
#     time.sleep(1)
#     print("正在爬取:%s"%link)     # 每一次执行的时候给一个输出：正在爬取哪个页面（0,25,50,75...）第一个link是0，第二个link是25...
#     with urlopen(link) as html:
#         text = html.read().decode("utf-8")        # 文本格式的html
#     doc = etree.HTML(text)        # 转换成dom对象
#
#     titles = doc.xpath("//ol[@class='grid_view']/li/div/div[@class='info']/div[@class='hd']/a/span[1]/text()")
#     links = doc.xpath("//ol[@class='grid_view']/li/div/div[@class='info']/div[@class='hd']/a/@href")
#
#     arr.append(list(zip(titles,links)))       # zip就是一一对应，然后转换成列表，然后存储下来，需要一个全局的arr放入内容，调用arr.append推送
#
# for link in urls:
#     spider(link)      # 循环执行link
#
# with open("./top250.txt",'wb') as f:        # 在当前的文件中打开一个top.250文件，以二进制写入的方式
#     pickle.dump(arr,f)        # 把arr存储到f中，内容就被写过来了


# with open("./top250.txt",'rb') as f:        # 以二进制读的形式打开
#     obj = pickle.load(f)      # 把f文件中的内容加载出来
#
# print(len(obj))     # 一共10页
#
# for item in obj:
#     print(item)     # 一个大列表，里边10个小列表
```
```
### 爬取豆瓣电影详情
from urllib.request import *
import pickle,fake_useragent,xlwt
from lxml.etree import *
import time

with open('top250.txt','rb') as f:
    arr = pickle.load(f)

lists = []
for arr1 in arr:
    for title,url in arr1:
        lists.append(url)

ua = fake_useragent.UserAgent()
header = {
    'User-Agent':ua.random
}

def spider(url):
    time.sleep(1)
    req = Request(url,headers=header)
    with urlopen(req) as html:
        text = html.read().decode()
    doc = HTML(text)
    # 导演
    pl1 = "/".join(doc.xpath("//div[@id='info']/span[1]/span[@class='attrs']/a[1]/text()"))
    # 编剧
    pl2 = "/".join(doc.xpath("//div[@id='info']/span[2]/span[@class='attrs']/a/text()"))
    # 主演
    pl3 = "/".join(doc.xpath("//div[@id='info']/span[3]/span[@class='attrs']/a/text()"))
    # 类型
    pl4 = "/".join(doc.xpath("//div[@id='info']/span[@property='v:genre']/text()"))
    # 剧情简介
    pl5 = doc.xpath("//span[@property='v:summary']/text()")[0].strip()
    print("正在爬取:%s" % pl1)
    return {
        # 'dir':pl1,
        # 'edit':pl2,
        # 'actor':pl3,
        # 'type':pl4,
        # 'dis':pl5
         pl1,
         pl2,
         pl3,
         pl4,
         pl5
    }

for url in lists:
    res = list(spider(url))
    print(res)
```